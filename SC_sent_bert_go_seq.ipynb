{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efc200be3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(words4sent, max_seq_len, feature_dim,   to_reverse=0):\n",
    "    length = []\n",
    "    output = []\n",
    "    \n",
    "    for words in words4sent:\n",
    "        if to_reverse:\n",
    "            words = np.flip(words, 0)\n",
    "        length.append( words.shape[0])\n",
    "        if  words.shape[0] < max_seq_len:\n",
    "            wordList = np.concatenate([words,np.zeros([max_seq_len - words.shape[0],feature_dim])])\n",
    "        output.append(wordList)\n",
    "    return np.array(output),np.array(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(x, lengths):\n",
    "    out = torch.FloatTensor(x.size(1), x.size(2)).zero_() # BxF\n",
    "    for i in range(x.size(1)):\n",
    "        out[i] = torch.mean(x[:lengths[i],i,:], 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RandLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_layers, output_dim,  bidirectional=False):\n",
    "        super(RandLSTM, self).__init__()\n",
    "        \n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.max_seq_len = 128\n",
    "        self.input_dim = input_dim\n",
    "         \n",
    "\n",
    "        self.e_hid_init = torch.zeros(1, 1, output_dim)\n",
    "        self.e_cell_init = torch.zeros(1, 1, output_dim)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lm = nn.LSTM(input_dim, output_dim, num_layers=num_layers,\n",
    "                          bidirectional= self.bidirectional, batch_first=True)\n",
    "\n",
    "        self.bidirectional += 1\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "   \n",
    "\n",
    "    def lstm(self, inputs, lengths):\n",
    "        bsz, max_len, _ = inputs.size()\n",
    "        in_embs = inputs\n",
    "        lens, indices = torch.sort(lengths, 0, True)\n",
    "\n",
    "        e_hid_init = self.e_hid_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        e_cell_init = self.e_cell_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        all_hids, (enc_last_hid, _) = self.lm(pack(in_embs[indices],\n",
    "                                                        lens.tolist(), batch_first=True), (e_hid_init, e_cell_init))\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        all_hids = unpack(all_hids, batch_first=True)[0][_indices]\n",
    "\n",
    "        return all_hids\n",
    "\n",
    "    def forward(self, words4sent):\n",
    "        \n",
    "        out, lengths = gen_embedding(words4sent, self.max_seq_len, self.input_dim)\n",
    "        out = torch.from_numpy(out).float()\n",
    "        lengths = torch.from_numpy(np.array(lengths))\n",
    "        out = self.lstm(out, lengths)\n",
    "#         print(\"output size:\",out.size())\n",
    "        out = out.transpose(1,0)\n",
    "        out = mean_pool(out, lengths)\n",
    "        return out\n",
    "\n",
    "    def encode(self, batch):\n",
    "        return self.forward(batch).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "prot2emb_w2v = {}\n",
    "project_dim = 1024\n",
    "num_layers = 1\n",
    "max_go_len = 1024\n",
    "max_protlen = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot2emb_bert = {}\n",
    "max_protlen = 0\n",
    "input_dim = 768\n",
    " \n",
    "bertlstm = RandLSTM(input_dim,num_layers,  project_dim, bidirectional = False)\n",
    "for key, value in protein2go.items():\n",
    "     \n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            words4sent.append(feature)\n",
    "        count += feature.shape[0] \n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = bertlstm.encode(words4sent)\n",
    "    else:\n",
    "        sent_embedding = np.zeros((1, project_dim))\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_bert[key] = sent_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "\n",
    "max_seq_len = 1000\n",
    "# max_protlen = 32\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "        input_dim = 768\n",
    "        num_layers = 1\n",
    "         \n",
    "        \n",
    "        \n",
    "        self.projection_dim = project_dim\n",
    "        self.bert_len = bert_len\n",
    " \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2emb_bert =  prot2emb_bert\n",
    "         \n",
    " \n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "    \n",
    "  \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_go1 = np.empty((self.batch_size, self.bert_len,self.projection_dim))\n",
    "        X_go2 = np.empty((self.batch_size, self.bert_len,self.projection_dim))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "          \n",
    "            \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "         \n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.bert_len,self.projection_dim))\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.bert_len,self.projection_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "               \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           (None, 32, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 32, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_43 (InputLayer)           (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_241 (Conv1D)             (None, 32, 32)       98336       input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_243 (Conv1D)             (None, 32, 32)       32800       input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_247 (Conv1D)             (None, 32, 32)       98336       input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_249 (Conv1D)             (None, 32, 32)       32800       input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_253 (Conv1D)             (None, 1000, 16)     976         input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_255 (Conv1D)             (None, 1000, 16)     336         input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_259 (Conv1D)             (None, 1000, 16)     976         input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_261 (Conv1D)             (None, 1000, 16)     336         input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_242 (Conv1D)             (None, 32, 32)       5152        conv1d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_244 (Conv1D)             (None, 32, 32)       3104        conv1d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_245 (Conv1D)             (None, 32, 32)       98336       input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_246 (Conv1D)             (None, 32, 32)       32800       input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_248 (Conv1D)             (None, 32, 32)       5152        conv1d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_250 (Conv1D)             (None, 32, 32)       3104        conv1d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_251 (Conv1D)             (None, 32, 32)       98336       input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_252 (Conv1D)             (None, 32, 32)       32800       input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_254 (Conv1D)             (None, 1000, 16)     1296        conv1d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_256 (Conv1D)             (None, 1000, 16)     784         conv1d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_257 (Conv1D)             (None, 1000, 16)     976         input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_258 (Conv1D)             (None, 1000, 16)     336         input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_260 (Conv1D)             (None, 1000, 16)     1296        conv1d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_262 (Conv1D)             (None, 1000, 16)     784         conv1d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_263 (Conv1D)             (None, 1000, 16)     976         input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_264 (Conv1D)             (None, 1000, 16)     336         input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 32, 128)      0           conv1d_242[0][0]                 \n",
      "                                                                 conv1d_244[0][0]                 \n",
      "                                                                 conv1d_245[0][0]                 \n",
      "                                                                 conv1d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 32, 128)      0           conv1d_248[0][0]                 \n",
      "                                                                 conv1d_250[0][0]                 \n",
      "                                                                 conv1d_251[0][0]                 \n",
      "                                                                 conv1d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 1000, 64)     0           conv1d_254[0][0]                 \n",
      "                                                                 conv1d_256[0][0]                 \n",
      "                                                                 conv1d_257[0][0]                 \n",
      "                                                                 conv1d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 1000, 64)     0           conv1d_260[0][0]                 \n",
      "                                                                 conv1d_262[0][0]                 \n",
      "                                                                 conv1d_263[0][0]                 \n",
      "                                                                 conv1d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 32, 128)      0           concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 32, 128)      0           concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 1000, 64)     0           concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 1000, 64)     0           concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_73 (Gl (None, 128)          0           dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_73 (Global (None, 128)          0           dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_73 (Attention)        (None, 128)          160         dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_74 (Gl (None, 128)          0           dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_74 (Global (None, 128)          0           dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_74 (Attention)        (None, 128)          160         dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_75 (Gl (None, 64)           0           dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_75 (Global (None, 64)           0           dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_75 (Attention)        (None, 64)           1064        dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_76 (Gl (None, 64)           0           dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_76 (Global (None, 64)           0           dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_76 (Attention)        (None, 64)           1064        dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 384)          0           global_average_pooling1d_73[0][0]\n",
      "                                                                 global_max_pooling1d_73[0][0]    \n",
      "                                                                 attention_73[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 384)          0           global_average_pooling1d_74[0][0]\n",
      "                                                                 global_max_pooling1d_74[0][0]    \n",
      "                                                                 attention_74[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 192)          0           global_average_pooling1d_75[0][0]\n",
      "                                                                 global_max_pooling1d_75[0][0]    \n",
      "                                                                 attention_75[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 192)          0           global_average_pooling1d_76[0][0]\n",
      "                                                                 global_max_pooling1d_76[0][0]    \n",
      "                                                                 attention_76[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 256)          98560       concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 256)          98560       concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 256)          49408       concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 256)          49408       concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 1024)         0           dense_81[0][0]                   \n",
      "                                                                 dense_82[0][0]                   \n",
      "                                                                 dense_83[0][0]                   \n",
      "                                                                 dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 1024)         1049600     concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 1024)         0           dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 1024)         1049600     dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 1024)         0           dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 512)          524800      dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 1)            513         dense_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 1)            0           dense_88[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,473,361\n",
      "Trainable params: 3,473,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([ x_a, x_b, x_c, x_gru_c, x_gru_b,  x_gru_a])\n",
    "    x = Dense(256)(x)\n",
    "    return x\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x\n",
    " \n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(bert_len,project_dim))\n",
    "    right_input_go = Input(shape=(bert_len,project_dim))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "    left_x_go = build_cnn_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_model(right_input_go, con_filters,gru_units)\n",
    "    left_x_seq = build_cnn_model(left_input_seq, con_filters//2, gru_units)\n",
    "    right_x_seq = build_cnn_model(right_input_seq, con_filters//2, gru_units)\n",
    "\n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "135/135 [==============================] - 29s 217ms/step - loss: 0.6137 - acc: 0.6758 - val_loss: 0.4496 - val_acc: 0.8099\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 17s 127ms/step - loss: 0.4073 - acc: 0.8185 - val_loss: 0.4255 - val_acc: 0.8060\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.3616 - acc: 0.8387 - val_loss: 0.2907 - val_acc: 0.8880\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.2763 - acc: 0.8848 - val_loss: 0.2568 - val_acc: 0.8991\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.2591 - acc: 0.8952 - val_loss: 0.2284 - val_acc: 0.9222\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 18s 130ms/step - loss: 0.1888 - acc: 0.9274 - val_loss: 0.1880 - val_acc: 0.9341\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 17s 128ms/step - loss: 0.2208 - acc: 0.9106 - val_loss: 0.1959 - val_acc: 0.9250\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.1648 - acc: 0.9396 - val_loss: 0.1911 - val_acc: 0.9317\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 17s 127ms/step - loss: 0.1700 - acc: 0.9355 - val_loss: 0.1641 - val_acc: 0.9358\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 17s 125ms/step - loss: 0.1738 - acc: 0.9327 - val_loss: 0.1784 - val_acc: 0.9297\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.1666 - acc: 0.9364 - val_loss: 0.2029 - val_acc: 0.9219\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 18s 130ms/step - loss: 0.1438 - acc: 0.9452 - val_loss: 0.1546 - val_acc: 0.9418\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 18s 134ms/step - loss: 0.1361 - acc: 0.9504 - val_loss: 0.1500 - val_acc: 0.9435\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 18s 134ms/step - loss: 0.1506 - acc: 0.9424 - val_loss: 0.1539 - val_acc: 0.9453\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 19s 139ms/step - loss: 0.1285 - acc: 0.9510 - val_loss: 0.1899 - val_acc: 0.9336\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 18s 132ms/step - loss: 0.1285 - acc: 0.9513 - val_loss: 0.1636 - val_acc: 0.9381\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 17s 129ms/step - loss: 0.1149 - acc: 0.9567 - val_loss: 0.1652 - val_acc: 0.9406\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 18s 132ms/step - loss: 0.1291 - acc: 0.9503 - val_loss: 0.1433 - val_acc: 0.9458\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 17s 126ms/step - loss: 0.1217 - acc: 0.9546 - val_loss: 0.2191 - val_acc: 0.9237\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 16s 120ms/step - loss: 0.1175 - acc: 0.9539 - val_loss: 0.1826 - val_acc: 0.9405\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 15s 107ms/step - loss: 0.1040 - acc: 0.9609 - val_loss: 0.1777 - val_acc: 0.9356\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 14s 102ms/step - loss: 0.0953 - acc: 0.9652 - val_loss: 0.1622 - val_acc: 0.9409\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 13s 99ms/step - loss: 0.0881 - acc: 0.9660 - val_loss: 0.1751 - val_acc: 0.9381\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 14s 106ms/step - loss: 0.1206 - acc: 0.9547 - val_loss: 0.1694 - val_acc: 0.9386\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 17s 124ms/step - loss: 0.1066 - acc: 0.9615 - val_loss: 0.1382 - val_acc: 0.9506\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 14s 104ms/step - loss: 0.1080 - acc: 0.9591 - val_loss: 0.1583 - val_acc: 0.9439\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 15s 109ms/step - loss: 0.0964 - acc: 0.9630 - val_loss: 0.1881 - val_acc: 0.9308\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 18s 130ms/step - loss: 0.0909 - acc: 0.9663 - val_loss: 0.1429 - val_acc: 0.9465\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.1037 - acc: 0.9603 - val_loss: 0.1483 - val_acc: 0.9461\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 19s 138ms/step - loss: 0.0939 - acc: 0.9653 - val_loss: 0.1844 - val_acc: 0.9397\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 19s 140ms/step - loss: 0.0880 - acc: 0.9669 - val_loss: 0.1850 - val_acc: 0.9384\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 21s 152ms/step - loss: 0.0803 - acc: 0.9674 - val_loss: 0.1550 - val_acc: 0.9461\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 20s 146ms/step - loss: 0.1022 - acc: 0.9624 - val_loss: 0.1543 - val_acc: 0.9457\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 20s 146ms/step - loss: 0.0747 - acc: 0.9718 - val_loss: 0.2639 - val_acc: 0.9257\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 20s 148ms/step - loss: 0.0882 - acc: 0.9680 - val_loss: 0.1488 - val_acc: 0.9431\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986201\n",
      "ACC: 0.953523\n",
      "MCC : 0.908220\n",
      "TPR:0.927624\n",
      "FPR:0.020780\n",
      "Pre:0.977920\n",
      "F1:0.952108\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.986201\n",
      "mean ACC: 0.953523\n",
      "mean MCC : 0.908220\n",
      "mean TPR:0.927624\n",
      "mean FPR:0.020780\n",
      "mean Pre:0.977920\n",
      "mean F1:0.952108\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 29s 215ms/step - loss: 0.5173 - acc: 0.7365 - val_loss: 0.5363 - val_acc: 0.7462\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.4774 - acc: 0.7871 - val_loss: 0.3313 - val_acc: 0.8655\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 19s 138ms/step - loss: 0.3971 - acc: 0.8200 - val_loss: 0.3941 - val_acc: 0.8404\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 17s 127ms/step - loss: 0.2843 - acc: 0.8799 - val_loss: 0.2394 - val_acc: 0.9100\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 16s 121ms/step - loss: 0.2837 - acc: 0.8828 - val_loss: 0.3454 - val_acc: 0.8501\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 19s 139ms/step - loss: 0.2018 - acc: 0.9218 - val_loss: 0.2476 - val_acc: 0.8995\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 19s 138ms/step - loss: 0.1870 - acc: 0.9273 - val_loss: 0.2235 - val_acc: 0.9147\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.2079 - acc: 0.9171 - val_loss: 0.2083 - val_acc: 0.9245\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 18s 130ms/step - loss: 0.1884 - acc: 0.9272 - val_loss: 0.1982 - val_acc: 0.9255\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 18s 132ms/step - loss: 0.1604 - acc: 0.9391 - val_loss: 0.2053 - val_acc: 0.9213\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 19s 144ms/step - loss: 0.1609 - acc: 0.9379 - val_loss: 0.1741 - val_acc: 0.9353\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 18s 136ms/step - loss: 0.1330 - acc: 0.9486 - val_loss: 0.1715 - val_acc: 0.9335\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 17s 129ms/step - loss: 0.1277 - acc: 0.9510 - val_loss: 0.1770 - val_acc: 0.9317\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 18s 135ms/step - loss: 0.1554 - acc: 0.9394 - val_loss: 0.1872 - val_acc: 0.9293\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.1277 - acc: 0.9524 - val_loss: 0.1757 - val_acc: 0.9344\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 19s 140ms/step - loss: 0.1284 - acc: 0.9509 - val_loss: 0.1905 - val_acc: 0.9225\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 19s 137ms/step - loss: 0.1325 - acc: 0.9491 - val_loss: 0.1532 - val_acc: 0.9428\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 18s 136ms/step - loss: 0.1126 - acc: 0.9576 - val_loss: 0.1806 - val_acc: 0.9310\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.1120 - acc: 0.9580 - val_loss: 0.1703 - val_acc: 0.9356\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 18s 131ms/step - loss: 0.1209 - acc: 0.9548 - val_loss: 0.1635 - val_acc: 0.9371\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 19s 138ms/step - loss: 0.1224 - acc: 0.9524 - val_loss: 0.1515 - val_acc: 0.9446\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.1205 - acc: 0.9546 - val_loss: 0.1581 - val_acc: 0.9435\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 18s 132ms/step - loss: 0.1117 - acc: 0.9572 - val_loss: 0.1503 - val_acc: 0.9428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "135/135 [==============================] - 18s 130ms/step - loss: 0.1260 - acc: 0.9511 - val_loss: 0.2340 - val_acc: 0.9164\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 18s 133ms/step - loss: 0.0935 - acc: 0.9643 - val_loss: 0.1638 - val_acc: 0.9403\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 17s 129ms/step - loss: 0.1048 - acc: 0.9600 - val_loss: 0.1577 - val_acc: 0.9446\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 18s 134ms/step - loss: 0.0949 - acc: 0.9645 - val_loss: 0.1553 - val_acc: 0.9434\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 17s 127ms/step - loss: 0.1121 - acc: 0.9566 - val_loss: 0.2104 - val_acc: 0.9200\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 18s 134ms/step - loss: 0.1240 - acc: 0.9537 - val_loss: 0.4453 - val_acc: 0.8213\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 18s 134ms/step - loss: 0.2189 - acc: 0.9153 - val_loss: 0.1545 - val_acc: 0.9433\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 16s 121ms/step - loss: 0.0837 - acc: 0.9681 - val_loss: 0.1594 - val_acc: 0.9405\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987167\n",
      "ACC: 0.954682\n",
      "MCC : 0.909875\n",
      "TPR:0.939380\n",
      "FPR:0.029481\n",
      "Pre:0.970567\n",
      "F1:0.954719\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.987167\n",
      "mean ACC: 0.954682\n",
      "mean MCC : 0.909875\n",
      "mean TPR:0.939380\n",
      "mean FPR:0.029481\n",
      "mean Pre:0.970567\n",
      "mean F1:0.954719\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 33s 246ms/step - loss: 0.5129 - acc: 0.7436 - val_loss: 0.4422 - val_acc: 0.7887\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 15s 115ms/step - loss: 0.3252 - acc: 0.8630 - val_loss: 0.2412 - val_acc: 0.9084\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 16s 117ms/step - loss: 0.4437 - acc: 0.7988 - val_loss: 0.3746 - val_acc: 0.8438\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 16s 116ms/step - loss: 0.2435 - acc: 0.9069 - val_loss: 0.2622 - val_acc: 0.8971\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 16s 119ms/step - loss: 0.2908 - acc: 0.8791 - val_loss: 0.5229 - val_acc: 0.7947\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 17s 125ms/step - loss: 0.3091 - acc: 0.8714 - val_loss: 0.2767 - val_acc: 0.8784\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 19s 142ms/step - loss: 0.2325 - acc: 0.9083 - val_loss: 0.4243 - val_acc: 0.8267\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 15s 114ms/step - loss: 0.2304 - acc: 0.9138 - val_loss: 0.1913 - val_acc: 0.9242\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 15s 113ms/step - loss: 0.2891 - acc: 0.8817 - val_loss: 0.2565 - val_acc: 0.8994\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 16s 118ms/step - loss: 0.1608 - acc: 0.9382 - val_loss: 0.1750 - val_acc: 0.9328\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 19s 138ms/step - loss: 0.2073 - acc: 0.9184 - val_loss: 0.2020 - val_acc: 0.9281\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.1724 - acc: 0.9322 - val_loss: 0.1502 - val_acc: 0.9461\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 19s 144ms/step - loss: 0.1303 - acc: 0.9509 - val_loss: 0.1607 - val_acc: 0.9392\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 20s 147ms/step - loss: 0.1294 - acc: 0.9532 - val_loss: 0.1867 - val_acc: 0.9354\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 20s 148ms/step - loss: 0.1287 - acc: 0.9511 - val_loss: 0.1550 - val_acc: 0.9433\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 21s 152ms/step - loss: 0.1796 - acc: 0.9320 - val_loss: 0.1993 - val_acc: 0.9344\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.1587 - acc: 0.9398 - val_loss: 0.1518 - val_acc: 0.9447\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 20s 148ms/step - loss: 0.1471 - acc: 0.9443 - val_loss: 0.2783 - val_acc: 0.8917\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 21s 157ms/step - loss: 0.1143 - acc: 0.9578 - val_loss: 0.1750 - val_acc: 0.9312\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 21s 154ms/step - loss: 0.1320 - acc: 0.9495 - val_loss: 0.1813 - val_acc: 0.9268\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 21s 157ms/step - loss: 0.1285 - acc: 0.9517 - val_loss: 0.1580 - val_acc: 0.9459\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.1421 - acc: 0.9455 - val_loss: 0.1659 - val_acc: 0.9397\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.984842\n",
      "ACC: 0.943324\n",
      "MCC : 0.889960\n",
      "TPR:0.899277\n",
      "FPR:0.013137\n",
      "Pre:0.985437\n",
      "F1:0.940388\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.984842\n",
      "mean ACC: 0.943324\n",
      "mean MCC : 0.889960\n",
      "mean TPR:0.899277\n",
      "mean FPR:0.013137\n",
      "mean Pre:0.985437\n",
      "mean F1:0.940388\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 42s 312ms/step - loss: 0.5765 - acc: 0.7007 - val_loss: 0.4730 - val_acc: 0.7652\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 21s 156ms/step - loss: 0.4087 - acc: 0.8160 - val_loss: 0.3942 - val_acc: 0.8357\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 20s 151ms/step - loss: 0.3212 - acc: 0.8653 - val_loss: 0.3565 - val_acc: 0.8639\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 21s 155ms/step - loss: 0.2966 - acc: 0.8785 - val_loss: 0.3176 - val_acc: 0.8906\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 20s 152ms/step - loss: 0.2025 - acc: 0.9215 - val_loss: 0.2059 - val_acc: 0.9223\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 21s 152ms/step - loss: 0.2409 - acc: 0.9016 - val_loss: 0.2209 - val_acc: 0.9235\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 20s 150ms/step - loss: 0.1863 - acc: 0.9295 - val_loss: 0.1928 - val_acc: 0.9237\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 19s 140ms/step - loss: 0.2055 - acc: 0.9168 - val_loss: 0.2375 - val_acc: 0.9020\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 19s 144ms/step - loss: 0.2526 - acc: 0.8968 - val_loss: 0.3745 - val_acc: 0.8662\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 20s 150ms/step - loss: 0.2404 - acc: 0.9004 - val_loss: 0.1837 - val_acc: 0.9348\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 20s 148ms/step - loss: 0.1392 - acc: 0.9467 - val_loss: 0.1656 - val_acc: 0.9366\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 20s 151ms/step - loss: 0.1460 - acc: 0.9456 - val_loss: 0.2054 - val_acc: 0.9220\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 21s 159ms/step - loss: 0.2155 - acc: 0.9150 - val_loss: 0.1728 - val_acc: 0.9329\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 20s 152ms/step - loss: 0.1487 - acc: 0.9450 - val_loss: 0.1879 - val_acc: 0.9291\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 21s 154ms/step - loss: 0.1990 - acc: 0.9246 - val_loss: 0.2032 - val_acc: 0.9152\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.1476 - acc: 0.9432 - val_loss: 0.1963 - val_acc: 0.9271\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 20s 147ms/step - loss: 0.1222 - acc: 0.9545 - val_loss: 0.1514 - val_acc: 0.9401\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 20s 147ms/step - loss: 0.1172 - acc: 0.9565 - val_loss: 0.1667 - val_acc: 0.9363\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 21s 155ms/step - loss: 0.1150 - acc: 0.9557 - val_loss: 0.1471 - val_acc: 0.9441\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.1281 - acc: 0.9514 - val_loss: 0.1562 - val_acc: 0.9421\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 17s 125ms/step - loss: 0.1150 - acc: 0.9557 - val_loss: 0.2103 - val_acc: 0.9168\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 17s 126ms/step - loss: 0.1129 - acc: 0.9583 - val_loss: 0.1418 - val_acc: 0.9480\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 14s 105ms/step - loss: 0.1138 - acc: 0.9576 - val_loss: 0.1482 - val_acc: 0.9420\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 15s 110ms/step - loss: 0.1133 - acc: 0.9558 - val_loss: 0.2072 - val_acc: 0.9273\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 15s 110ms/step - loss: 0.1089 - acc: 0.9592 - val_loss: 0.1780 - val_acc: 0.9367\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 16s 117ms/step - loss: 0.0997 - acc: 0.9617 - val_loss: 0.1558 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "135/135 [==============================] - 16s 118ms/step - loss: 0.0899 - acc: 0.9658 - val_loss: 0.1529 - val_acc: 0.9440\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 20s 147ms/step - loss: 0.0888 - acc: 0.9668 - val_loss: 0.1522 - val_acc: 0.9400\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 17s 128ms/step - loss: 0.1035 - acc: 0.9608 - val_loss: 0.1412 - val_acc: 0.9457\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 16s 117ms/step - loss: 0.0811 - acc: 0.9678 - val_loss: 0.1452 - val_acc: 0.9451\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 16s 121ms/step - loss: 0.0960 - acc: 0.9644 - val_loss: 0.1456 - val_acc: 0.9427\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 20s 152ms/step - loss: 0.0742 - acc: 0.9724 - val_loss: 0.1838 - val_acc: 0.9369\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986756\n",
      "ACC: 0.951669\n",
      "MCC : 0.903676\n",
      "TPR:0.939186\n",
      "FPR:0.035496\n",
      "Pre:0.964546\n",
      "F1:0.951697\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.986756\n",
      "mean ACC: 0.951669\n",
      "mean MCC : 0.903676\n",
      "mean TPR:0.939186\n",
      "mean FPR:0.035496\n",
      "mean Pre:0.964546\n",
      "mean F1:0.951697\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 44s 326ms/step - loss: 0.6224 - acc: 0.6548 - val_loss: 0.5088 - val_acc: 0.7856\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 21s 157ms/step - loss: 0.4585 - acc: 0.7905 - val_loss: 0.3944 - val_acc: 0.8241\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.3646 - acc: 0.8427 - val_loss: 0.2731 - val_acc: 0.8939\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.2557 - acc: 0.8976 - val_loss: 0.2791 - val_acc: 0.8983\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 21s 159ms/step - loss: 0.2070 - acc: 0.9193 - val_loss: 0.2121 - val_acc: 0.9176\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.2097 - acc: 0.9183 - val_loss: 0.1857 - val_acc: 0.9285\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 22s 165ms/step - loss: 0.1964 - acc: 0.9241 - val_loss: 0.1730 - val_acc: 0.9331\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 22s 164ms/step - loss: 0.1656 - acc: 0.9369 - val_loss: 0.1675 - val_acc: 0.9353\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 21s 159ms/step - loss: 0.1563 - acc: 0.9383 - val_loss: 0.1754 - val_acc: 0.9319\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 22s 160ms/step - loss: 0.1509 - acc: 0.9426 - val_loss: 0.2222 - val_acc: 0.9173\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 22s 161ms/step - loss: 0.1241 - acc: 0.9523 - val_loss: 0.1931 - val_acc: 0.9206\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 23s 167ms/step - loss: 0.1437 - acc: 0.9448 - val_loss: 0.1600 - val_acc: 0.9407\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 22s 160ms/step - loss: 0.1313 - acc: 0.9489 - val_loss: 0.1844 - val_acc: 0.9293\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 22s 161ms/step - loss: 0.1525 - acc: 0.9422 - val_loss: 0.1673 - val_acc: 0.9427\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.1131 - acc: 0.9580 - val_loss: 0.1592 - val_acc: 0.9393\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 21s 159ms/step - loss: 0.1263 - acc: 0.9509 - val_loss: 0.2035 - val_acc: 0.9225\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 21s 159ms/step - loss: 0.1089 - acc: 0.9597 - val_loss: 0.1719 - val_acc: 0.9332\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 21s 156ms/step - loss: 0.1109 - acc: 0.9578 - val_loss: 0.1679 - val_acc: 0.9418\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 21s 155ms/step - loss: 0.1315 - acc: 0.9484 - val_loss: 0.1718 - val_acc: 0.9292\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 21s 157ms/step - loss: 0.1328 - acc: 0.9487 - val_loss: 0.1776 - val_acc: 0.9357\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 22s 166ms/step - loss: 0.1095 - acc: 0.9593 - val_loss: 0.1388 - val_acc: 0.9492\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.1114 - acc: 0.9576 - val_loss: 0.1577 - val_acc: 0.9415\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 21s 155ms/step - loss: 0.1075 - acc: 0.9601 - val_loss: 0.2110 - val_acc: 0.9208\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 21s 155ms/step - loss: 0.0865 - acc: 0.9674 - val_loss: 0.1972 - val_acc: 0.9239\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.1100 - acc: 0.9580 - val_loss: 0.1981 - val_acc: 0.9285\n",
      "Epoch 26/100\n",
      "134/135 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9628"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "dataset_name = 'sc'\n",
    "for rep in range(5):\n",
    "    n_splits = 1\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'SC_CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'SC_CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'SC_CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 128\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'SC_CV/bert_sent_GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        #  max_queue_size=16, workers=8, use_multiprocessing=True,\n",
    "        # validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint] \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    epochs = 100,verbose=1,validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint])\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('bert_sent_seq_and_go__incep_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.987496\n",
      "mean ACC: 0.956189\n",
      "mean MCC : 0.913217\n",
      "mean TPR:0.935106\n",
      "mean FPR:0.022618\n",
      "mean Pre:0.976558\n",
      "mean F1:0.955375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'bert_sent_seq_and_go__incep_0.npz')\n",
    "results2 =   np.load( 'bert_sent_seq_and_go__incep_1.npz')\n",
    "results3 =   np.load( 'bert_sent_seq_and_go__incep_2.npz')\n",
    "results4 =   np.load( 'bert_sent_seq_and_go__incep_3.npz')\n",
    "results5 =   np.load( 'bert_sent_seq_and_go__incep_4.npz')\n",
    "  \n",
    "\n",
    "print ('mean AUC: %f' %  (  (np.mean( results4[ 'AUCs' ] )  +  np.mean( results5[ 'AUCs' ] )  + np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/5     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.mean( results4[ 'ACCs' ] )  + np.mean(  results5[ 'ACCs' ] )  +   np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/5) )\n",
    "print ('mean MCC : %f' %  ( ( np.mean( results4[ 'MCCs' ] )  + np.mean(  results5[ 'MCCs' ] )  + np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/5))\n",
    "print('mean TPR:%f'%    (( np.mean( results4[ 'TPRs' ] )  + np.mean(  results5[ 'TPRs' ] )  + np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/5))\n",
    "print('mean FPR:%f'%   (  (np.mean( results4[ 'FPRs' ] )  + np.mean(  results5[ 'FPRs' ] )  + np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/5))\n",
    "print('mean Pre:%f'%    ( (np.mean( results4[ 'Precs' ] )  + np.mean(  results5[ 'Precs' ] )  + np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/5))\n",
    "print('mean F1:%f'%    (  (np.mean( results4[ 'F1s' ] )  + np.mean(  results5[ 'F1s' ] )  +np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.000694\n",
      "mean ACC: 0.001835\n",
      "mean MCC : 0.003517\n",
      "mean TPR:0.004715\n",
      "mean FPR:0.001255\n",
      "mean Pre:0.001160\n",
      "mean F1:0.002342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'bert_sent_seq_and_go__incep_0.npz')\n",
    "results2 =   np.load( 'bert_sent_seq_and_go__incep_1.npz')\n",
    "results3 =   np.load( 'bert_sent_seq_and_go__incep_2.npz')\n",
    "results4 =   np.load( 'bert_sent_seq_and_go__incep_3.npz')\n",
    "results5 =   np.load( 'bert_sent_seq_and_go__incep_4.npz')\n",
    "  \n",
    "\n",
    "print ('mean AUC: %f' %  (  (np.std( np.concatenate( [results4[ 'AUCs' ], results3[ 'AUCs' ], results2[ 'AUCs' ],results1[ 'AUCs' ],results5[ 'AUCs' ]   ]  )     ))     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.std(np.concatenate( [results4[ 'ACCs' ], results3[ 'ACCs' ],results2[ 'ACCs' ],results1[ 'ACCs' ],results5[ 'ACCs' ] ]  ) ))) )\n",
    "print ('mean MCC : %f' %  ( ( np.std(  np.concatenate( [results4[ 'MCCs' ],results3[ 'MCCs' ],results2[ 'MCCs' ],results1[ 'MCCs' ],results5[ 'MCCs' ] ])  ))))\n",
    "print('mean TPR:%f'%    (( np.std( np.concatenate( [ results4[ 'TPRs' ],results3[ 'TPRs' ],results2[ 'TPRs' ],results1[ 'TPRs' ], results5[ 'TPRs' ]]) ))))\n",
    "print('mean FPR:%f'%   (  (np.std( np.concatenate( [ results4[ 'FPRs' ],results3[ 'FPRs' ],results2[ 'FPRs' ],results1[ 'FPRs' ], results5[ 'FPRs' ]])  )     )))\n",
    "print('mean Pre:%f'%    ( (np.std( np.concatenate( [ results4[ 'Precs' ],results3[ 'Precs' ],results2[ 'Precs' ],results1[ 'Precs' ],results5[ 'Precs' ]])   )     )))\n",
    "print('mean F1:%f'%    (  (np.std( np.concatenate( [results4[ 'F1s' ],results3[ 'F1s' ],results2[ 'F1s' ],results1[ 'F1s' ],results5[ 'F1s' ] ])      )     )))\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
