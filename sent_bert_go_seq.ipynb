{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc444110230>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(words4sent, max_seq_len, feature_dim,   to_reverse=0):\n",
    "    length = []\n",
    "    output = []\n",
    "    \n",
    "    for words in words4sent:\n",
    "        if to_reverse:\n",
    "            words = np.flip(words, 0)\n",
    "        length.append( words.shape[0])\n",
    "        if  words.shape[0] < max_seq_len:\n",
    "            wordList = np.concatenate([words,np.zeros([max_seq_len - words.shape[0],feature_dim])])\n",
    "        output.append(wordList)\n",
    "    return np.array(output),np.array(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(x, lengths):\n",
    "    out = torch.FloatTensor(x.size(1), x.size(2)).zero_() # BxF\n",
    "    for i in range(x.size(1)):\n",
    "        out[i] = torch.mean(x[:lengths[i],i,:], 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RandLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_layers, output_dim,  bidirectional=False):\n",
    "        super(RandLSTM, self).__init__()\n",
    "        \n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.max_seq_len = 128\n",
    "        self.input_dim = input_dim\n",
    "         \n",
    "\n",
    "        self.e_hid_init = torch.zeros(1, 1, output_dim)\n",
    "        self.e_cell_init = torch.zeros(1, 1, output_dim)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lm = nn.LSTM(input_dim, output_dim, num_layers=num_layers,\n",
    "                          bidirectional= self.bidirectional, batch_first=True)\n",
    "\n",
    "        self.bidirectional += 1\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "   \n",
    "\n",
    "    def lstm(self, inputs, lengths):\n",
    "        bsz, max_len, _ = inputs.size()\n",
    "        in_embs = inputs\n",
    "        lens, indices = torch.sort(lengths, 0, True)\n",
    "\n",
    "        e_hid_init = self.e_hid_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        e_cell_init = self.e_cell_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        all_hids, (enc_last_hid, _) = self.lm(pack(in_embs[indices],\n",
    "                                                        lens.tolist(), batch_first=True), (e_hid_init, e_cell_init))\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        all_hids = unpack(all_hids, batch_first=True)[0][_indices]\n",
    "\n",
    "        return all_hids\n",
    "\n",
    "    def forward(self, words4sent):\n",
    "        \n",
    "        out, lengths = gen_embedding(words4sent, self.max_seq_len, self.input_dim)\n",
    "        out = torch.from_numpy(out).float()\n",
    "        lengths = torch.from_numpy(np.array(lengths))\n",
    "        out = self.lstm(out, lengths)\n",
    "#         print(\"output size:\",out.size())\n",
    "        out = out.transpose(1,0)\n",
    "        out = mean_pool(out, lengths)\n",
    "        return out\n",
    "\n",
    "    def encode(self, batch):\n",
    "        return self.forward(batch).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract w2v model\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "w2vmodel =  Word2Vec.load('/home/xhh/PMC_model/PMC_model.txt')\n",
    "def vector_name(name): \n",
    "    s = name.split(' ')\n",
    "    vectors = [] \n",
    "    for word in s: \n",
    "        if w2vmodel.wv.__contains__(word):   \n",
    "            vectors.append(w2vmodel.wv[word]) \n",
    "    else: \n",
    "        clear_words = re.sub('[^A-Za-z0-9]+', ' ', word) \n",
    "        clear_words = clear_words.lstrip().rstrip().split(' ')\n",
    "        for w in clear_words: \n",
    "            if w2vmodel.wv.__contains__(w): \n",
    "                vectors.append(w2vmodel.wv[w]) \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    " \n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_name_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"name: \"):\n",
    "             name=line[len(\"name: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_name_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "protein2go =  load_dict('ATprot2go.pkl')\n",
    "prot2emb_w2v = {}\n",
    "project_dim = 1024\n",
    "num_layers = 1\n",
    "max_go_len = 1024\n",
    "max_protlen = 0\n",
    "w2vlstm = RandLSTM(200,num_layers,  project_dim, bidirectional = False)\n",
    "\n",
    "\n",
    "for key, value in protein2go.items(): \n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.array(vector_name(id_name_dicts[go]))\n",
    "            if feature.shape[0] > 0:\n",
    "                words4sent.append(feature)\n",
    "            \n",
    "        count += feature.shape[0]\n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = w2vlstm.encode(words4sent)\n",
    "    else:\n",
    "        sent_embedding = np.zeros((1, project_dim))\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_w2v[key] = sent_embedding \n",
    "\n",
    "del w2vmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(max_protlen)\n",
    "w2v_len = 64\n",
    "# w2v_len = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot2emb_bert = {}\n",
    "max_protlen = 0\n",
    "input_dim = 768\n",
    " \n",
    "bertlstm = RandLSTM(input_dim,num_layers,  project_dim, bidirectional = False)\n",
    "for key, value in protein2go.items():\n",
    "     \n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            words4sent.append(feature)\n",
    "        count += feature.shape[0] \n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = bertlstm.encode(words4sent)\n",
    "    else:\n",
    "        sent_embedding = np.zeros((1, project_dim))\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_bert[key] = sent_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(max_protlen)\n",
    "bert_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "\n",
    "max_seq_len = 1000\n",
    "# max_protlen = 32\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "        input_dim = 768\n",
    "        num_layers = 1\n",
    "         \n",
    "        \n",
    "        \n",
    "        self.projection_dim = project_dim\n",
    "        self.bert_len = bert_len\n",
    "        self.w2v_len = w2v_len\n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.protein2seq = load_dict('ATprot2seq.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2emb_bert =  prot2emb_bert\n",
    "        self.prot2emb_w2v = prot2emb_w2v\n",
    "         \n",
    "#         self.prot2embedding() \n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "    \n",
    "#     def prot2embedding(self):\n",
    "#         for key, value in self.protein2go.items():\n",
    "#             X_go1 =  np.zeros((1,768))\n",
    "#             allgos = value.split(',') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             words4sent = []\n",
    "#             for  go in  allgos:\n",
    "#                 if len(go) > 2:\n",
    "#                     feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                     words4sent.append(feature)\n",
    "#                 if count + feature.shape[0] > max_go_len:\n",
    "#                     break    \n",
    "#                 count += feature.shape[0] \n",
    "#             if len(words4sent) > 0:\n",
    "#                 sent_embedding = self.bertlstm.encode(words4sent)\n",
    "#             else:\n",
    "#                 sent_embedding = np.zeros((1, self.projection_dim))\n",
    "#             self.prot2emb[key] = sent_embedding \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "#         X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "#         X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_go1 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "#             X_seq1[i] =  self.protein2onehot[p1]\n",
    "#             X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "             \n",
    "            \n",
    "            \n",
    "        return [X_go1, X_go2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "#         X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))  \n",
    "#         X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "#             X_seq1[i] =  self.protein2onehot[p1]\n",
    "#             X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "           \n",
    "        return [X_go1, X_go2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encode_smiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3ae32b997421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3ae32b997421>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mleft_x_go\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_FILTERS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILTER_LENGTH1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_x_go\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mleft_x_go\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_FILTERS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILTER_LENGTH1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_x_go\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mleft_x_go\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_smiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pool_size=pool_length[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_smiles' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "class ResidualConv1D:\n",
    "    \"\"\"\n",
    "    ***ResidualConv1D for use with best performing classifier***\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, kernel_size, pool=False):\n",
    "        self.pool = pool\n",
    "        self.kernel_size = kernel_size\n",
    "        self.params = {\n",
    "            \"padding\": \"same\",\n",
    "            \"kernel_initializer\": \"he_uniform\",\n",
    "            \"strides\": 1,\n",
    "            \"filters\": filters,\n",
    "        }\n",
    "\n",
    "    def build(self, x):\n",
    "\n",
    "        res = x\n",
    "        if self.pool:\n",
    "            x = MaxPooling1D(1, padding=\"same\")(x)\n",
    "            res = Conv1D(kernel_size=1, **self.params)(res)\n",
    "\n",
    "        out = Conv1D(kernel_size=1, **self.params)(x)\n",
    "\n",
    "        out = BatchNormalization(momentum=0.9)(out)\n",
    "        out = Activation(\"relu\")(out)\n",
    "        out = Conv1D(kernel_size=self.kernel_size, **self.params)(out)\n",
    "\n",
    "        out = BatchNormalization(momentum=0.9)(out)\n",
    "        out = Activation(\"relu\")(out)\n",
    "        out = Conv1D(kernel_size=self.kernel_size, **self.params)(out)\n",
    "\n",
    "        out = keras.layers.add([res, out])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.build(x)\n",
    "\n",
    "\n",
    "def build_residual_cnn(input_x):\n",
    "    \n",
    "\n",
    "    x = Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=16,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "    )(input_x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # residual net part\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3, pool=True)(x)\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "    \n",
    "    \n",
    "    x = ResidualConv1D(filters=64, kernel_size=5, pool=True)(x)\n",
    "    x = ResidualConv1D(filters=64, kernel_size=5)(x)\n",
    "    x = ResidualConv1D(filters=64, kernel_size=5)(x)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "#     x = MaxPooling1D(1, padding=\"same\")(x)\n",
    "    x  = GlobalAveragePooling1D()(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_fc_model(input_x):\n",
    "     \n",
    "    \n",
    "    x_a = GlobalAveragePooling1D()(input_x)\n",
    "    x_b = GlobalMaxPooling1D()(input_x)\n",
    "#     x_c = Attention()(input_x)\n",
    "    x = Concatenate()([x_a, x_b])\n",
    "    x = Dense(1024)(x)\n",
    "#     x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    left_input_go = Input(shape=(bert_len+w2v_len,project_dim))\n",
    "    right_input_go = Input(shape=(bert_len+w2v_len,project_dim))\n",
    "    \n",
    "    \n",
    "    NUM_FILTERS = 16\n",
    "    FILTER_LENGTH1 = 3\n",
    "    FILTER_LENGTH2 = 3\n",
    "    \n",
    "    \n",
    "    left_x_go= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_input_go)\n",
    "    left_x_go = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_x_go)\n",
    "    left_x_go = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_x_go)\n",
    "    left_x_go = GlobalMaxPooling1D()(encode_smiles) #pool_size=pool_length[i]\n",
    "\n",
    "\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_input_go)\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_x_go)\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_x_go)\n",
    "    right_x_go = GlobalMaxPooling1D()(encode_protein)\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "#     left_x_go = build_residual_cnn(left_input_go)\n",
    "#     right_x_go = build_residual_cnn(right_input_go)\n",
    "     \n",
    "     \n",
    "    x =   Concatenate()([left_x_go  , right_x_go])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "dataset_name = 'AT'\n",
    "for rep in range(3):\n",
    "    n_splits = 10\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 64\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'CV/fusion_sent_GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        #  max_queue_size=16, workers=8, use_multiprocessing=True,\n",
    "        # validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint] \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    epochs = 100,verbose=1,validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint])\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('fusion_sent_seq_and_go'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'fusion_sent_seq_and_go__incep_0.npz')\n",
    "results2 =   np.load( 'fusion_sent_seq_and_go__incep_1.npz')\n",
    "results3 =   np.load( 'fusion_sent_seq_and_go__incep_2.npz')\n",
    "print ('mean AUC: %f' %  ( (np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/3     ) )\n",
    "print ('mean ACC: %f' %   ( (np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/3) )\n",
    "print ('mean MCC : %f' %  (  (np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/3))\n",
    "print('mean TPR:%f'%    ((np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/3))\n",
    "print('mean FPR:%f'%   ( (np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/3))\n",
    "print('mean Pre:%f'%    ((np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/3))\n",
    "print('mean F1:%f'%    ((np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257140295395118 0.87016835016835 0.7479247094254423 0.840650636445967 0.09735936163935063 0.8963718541880775 0.8616709106519065\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aucs = 0\n",
    "accs = 0\n",
    "mccs = 0\n",
    "tprs = 0\n",
    "fprs = 0\n",
    "pres = 0\n",
    "f1s  = 0\n",
    "for i in range(3):\n",
    "    for j in range(10):\n",
    "        results_file = 'CV/sent_fusion_'+str(i)+'-'+str(j)+'.npz'\n",
    "        results = np.load(results_file)\n",
    "        aucs += results['AUCs']\n",
    "        accs += results['ACCs']\n",
    "        mccs += results['MCCs']\n",
    "        tprs += results['TPRs']\n",
    "        fprs += results['FPRs']\n",
    "        pres += results['Precs']\n",
    "        f1s += results['F1s']\n",
    "        \n",
    "print(aucs/30, accs/30, mccs/30, tprs/30, fprs/30, pres/30, f1s/30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
